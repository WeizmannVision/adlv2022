<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<!-- saved from url=(0045)http://6.869.csail.mit.edu/fa19/schedule.html -->
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	<title>Advanced Topics In Computer Vision And Deep Learning</title>
	<link href="css/style.css" rel="stylesheet" type="text/css">
</head>

<body>

<div class="container">
	<table border="0" align="center">
	<tbody>
		<tr>
			<td width="160" align="right"><a href="https://www.weizmann.ac.il"><img src="pics/weizmann_logo.jpg" width="240" height="126" border="0" /></a></td>
			<td width="623" align="center" valign="middle"><h3>Weizmann Institute of Science</h3>
			<p><strong id="docs-internal-guid-be9d3617-7fff-49eb-ccd0-d78bcfc8d0b2">20214022</strong></p>
			<span class="title">Advanced Topics In Computer Vision <br> And Deep Learning</span></td>
			<td width="160" align="left"><a href="https://www.weizmann.ac.il/math/waic/home"><img src="pics/waic_logo.png" width="100" height="95" border="0" /></a></td>
		</tr>
		<tr><td colspan="3" align="center"><h3>Spring 2021</h3></td></tr>
		<tr>
			<td colspan="4" align="center">
			<span class="menubar"> [
			<a href="index.html">Home</a> | 
			<a href="schedule.html">Schedule</a> | 
			<a href="prerequisites.html">Prerequisites</a> | 
			<a href="useful.html">Useful Links</a>
			] </span></td>
		</tr>
	</tbody>
	</table>

	<p>	The full topics spreadsheet is available <a href="https://docs.google.com/spreadsheets/d/1nuasFdj_C2tgSootiF0V8JiiXWO2V0xfwRrw5yD1ZDk/edit?usp=sharing"> HERE </a> </p>
	<p>	The zoom recordings are only available with Weizmann credentials </p>
	
<table class="schedule" align="center" border="1" width="986">
  <tbody>
    <tr>
      <td align="center" width="80"><strong>Date</strong></td>
	  <td align="center" width="72"><strong>Speakers</strong></td>
      <td align="center" width="290"><strong>Papers</strong></td>
      <td align="center" width="40"><strong>Slides</strong></td>
      <td align="center" width="40"><strong>Zoom Recording</strong></td>
    </tr>

    <tr><td colspan="5" class="schedule_week" align="center" height="16" valign="middle">Introduction Meeting</td></tr>
	<tr>
    <td sdnum="1033;0;@" align="center">21/3/2021</td>
    <td align="left">Michal Irani <br> Niv Haim <br> Shai Bagon</td> 
    <td align="left" colspan="2">
		<a class="paper" href="http://www.wisdom.weizmann.ac.il/~/vision/courses/2021_2/ADLV/files/Intro_AdvancedCourse_2021.pptx"> Course Intro (and DOs and DON'Ts when giving a talk) </a><br>
		<a class="paper" href="http://www.wisdom.weizmann.ac.il/~/vision/courses/2021_2/ADLV/files/timeline-intro.pptx"> Talk Preparation Timeline and Guidelines </a><br>
		<a class="paper" href="http://www.wisdom.weizmann.ac.il/~/vision/courses/2021_2/ADLV/files/ADLV_2021_Zoom_instructions.pptx"> Zoom intro </a>
    </td>
	<td align="center"><a href="https://weizmann.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=89e82ec0-4f14-4567-82c3-acf200dcb614"> Zoom </a></td>
	</tr>

	<tr><td sdval="2" sdnum="1033;" align="center" height="16" colspan="100%">Passover</td></tr>

    <tr><td colspan="5" class="schedule_week" align="center" height="16" valign="middle">Attention/Transformers in Vision</td></tr>
	<tr>
    <td sdnum="1033;0;@" align="center">4/4/2021</td>
    <td align="left">Shir Amir <br> Shiran Zada</td> 
	<td> 
		<a href="https://arxiv.org/abs/2010.11929" class="paper"> An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale </a><br>
		<a href="https://epfml.github.io/attention-cnn/" class="paper"> On the Relationship between Self-Attention and Convolutional Layers </a><br>
		<a href="https://compvis.github.io/taming-transformers/" class="paper"> Taming Transformers for High-Resolution Image Synthesis </a><br>
		<a href="https://openreview.net/forum?id=rJgYQ4BlUH" class="paper"> Stand-Alone Self-Attention in Vision Models </a><br>
		<a href="https://arxiv.org/abs/2102.05095" class="paper"> Is Space-Time Attention All You Need for Video Understanding? </a><br>
		<a href="https://arxiv.org/abs/2012.12877" class="paper"> Training data-efficient image transformers & distillation through attention </a><br>
	</td>
    <td align="center"><a href="http://www.wisdom.weizmann.ac.il/~/vision/courses/2021_2/ADLV/files/Attention_in_CV_ADLV2021.pptx"> Slides </a></td>
	<td align="center"><a href="https://weizmann.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=33daa2ba-d542-4b56-92ca-ad0000e1fadc"> Zoom </a></td>
	</tr>
	
	<tr><td colspan="5" class="schedule_week" align="center" height="16" valign="middle">Advances in Reinforcement Learning</td></tr>
	<tr>
    <td sdnum="1033;0;@" align="center">11/4/2021</td>
    <td align="left">Hodaya Koslowski <br> Yuval Belfer</td> 
	<td> 
		<a class="paper" href="https://arxiv.org/abs/1911.08265"> Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model </a><br>
		<a class="paper" href="https://danijar.com/project/dreamerv2/"> Mastering Atari with Discrete World Models </a><br>
		<a class="paper" href="https://arxiv.org/abs/2007.13544"> Combining Deep Reinforcement Learning and Search for Imperfect-Information Games </a><br>
	</td>
	<td align="center"><a href="http://www.wisdom.weizmann.ac.il/~/vision/courses/2021_2/ADLV/files/Advanced_in_RL_Publish.pptx"> Slides </a></td>
	<td align="center"><a href="https://weizmann.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=d432f91e-4fc6-426e-9e48-ad0700daee77"> Zoom </a></td>
	</tr>

	<tr><td colspan="5" class="schedule_week" align="center" height="16" valign="middle">StyleGAN and Applications</td></tr>
	<tr>
    <td sdnum="1033;0;@" align="center">18/4/2021</td>
    <td align="left">Or bar-Shira <br> Oz Frank</td> 
	<td> 
		<a class="paper" href="https://arxiv.org/abs/1710.10196"> Progressive Growing of GANs for Improved Quality, Stability, and Variation </a><br>
		<a class="paper" href="https://arxiv.org/abs/1812.04948"> A Style-Based Generator Architecture for Generative Adversarial Networks </a><br>
		<a class="paper" href="https://arxiv.org/abs/1912.04958"> Stylegan2: Analyzing and Improving the Image Quality of StyleGAN </a><br>
		<a class="paper" href="https://arxiv.org/abs/2006.06676"> Stylegan-Ada: Training Generative Adversarial Networks with Limited Data </a><br>
		<a class="paper" href="https://genforce.github.io/sefa/"> Interpreting the Latent Space of GANs for Semantic Face Editing </a><br>
		<a class="paper" href="https://minyoungg.github.io/pix2latent/"> Transforming and Projecting Images into Class-conditional Generative Networks </a><br>
		<a class="paper" href="https://time-travel-rephotography.github.io/"> Time-Travel Rephotography </a><br>
		<a class="paper" href="https://saic-mdal.github.io/deep-landscape/"> DeepLandscape: Adversarial Modeling of Landscape Videos </a><br>	
	</td>
	<td align="center"><a href="http://www.wisdom.weizmann.ac.il/~/vision/courses/2021_2/ADLV/files/style_gans_and_applications_final.pptx"> Slides </a></td>
	<td align="center"><a href="https://weizmann.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=a0641d10-18e2-467e-80d6-ad0e00e599a8"> Zoom </a></td>
	</tr>

	<tr><td colspan="5" class="schedule_week" align="center" height="16" valign="middle">Image Synthesis without GANs</td></tr>
	<tr>
    <td sdnum="1033;0;@" align="center">25/4/2021</td>
    <td align="left">Itai Antebi <br> Rafail Fridman</td> 
	<td> 
		<a class="paper" href="https://rameenabdal.github.io/StyleFlow/"> StyleFlow: Attribute-conditioned Exploration of StyleGAN-Generated Images using Conditional Continuous Normalizing Flows </a><br>
		<a class="paper" href="https://arxiv.org/abs/2006.14200"> SRFlow: Learning the Super-Resolution Space with Normalizing Flow </a><br>
		<a class="paper" href="https://arxiv.org/abs/2007.03898"> NVAE: A Deep Hierarchical Variational Autoencoder </a><br>
		<a class="paper" href="https://arxiv.org/abs/1906.00446"> Generating Diverse High-Fidelity Images with VQ-VAE-2 </a><br>
		<a class="paper" href="https://arxiv.org/abs/1807.03039"> Glow: Generative Flow with Invertible 1x1 Convolutions </a><br>
	</td>
	<td align="center"><a href="http://www.wisdom.weizmann.ac.il/~/vision/courses/2021_2/ADLV/files/Image_Synthesis_Without_GANs.pptx"> Slides </a></td>
	<td align="center"><a href="https://weizmann.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=e463f327-1205-4ce7-b19d-ad1500ddf0c4"> Zoom </a></td>
	</tr>

	<tr><td colspan="5" class="schedule_week" align="center" height="16" valign="middle">Deep Learning meets Natural Sciences</td></tr>
	<tr>
    <td sdnum="1033;0;@" align="center">02/05/2021</td>
    <td align="left">Asaf Petruschka <br> Dror Bar</td> 
	<td> 
		<a class="paper" href="https://www.nature.com/articles/s41586-019-1923-7.epdf"> AlphaFold </a><br>
		<a class="paper" href="https://fabianfuchsml.github.io/alphafold2/"> AlphaFold 2 & Equivariance </a><br>
		<a class="paper" href="https://arxiv.org/abs/2006.15222"> BERTology Meets Biology: Interpreting Attention in Protein Language Models </a><br>
	</td>
	<td align="center"><a href="http://www.wisdom.weizmann.ac.il/~/vision/courses/2021_2/ADLV/files/Deep_Learning_meets_Natural_Sciences.pptx"> Slides </a></td>
	<td align="center"><a href="https://weizmann.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=95558ec2-b11f-4dce-8778-ad1c00dcf059"> Zoom </a></td>

	</tr>

	<tr><td colspan="5" class="schedule_week" align="center" height="16" valign="middle">Efficient Attention Mechanisms</td></tr>
	<tr>
    <td sdnum="1033;0;@" align="center">09/05/2021</td>
    <td align="left">Dana Joffe <br> Roy Abel</td> 
	<td> 
		<a class="paper" href="https://arxiv.org/abs/2001.04451"> Reformer: The Efficient Transformer </a><br>
		<a class="paper" href="https://arxiv.org/abs/2006.04768"> Linformer: Self-Attention with Linear Complexity </a><br>
		<a class="paper" href="https://arxiv.org/abs/2006.04768"> Rethinking Attention with Performers </a><br>
		<a class="paper" href="https://arxiv.org/abs/2003.05997"> Efficient Content-Based Sparse Attention with Routing Transformers </a><br>
		<a class="paper" href="https://arxiv.org/abs/2103.03206"> Perceiver: General Perception with Iterative Attention </a><br>
		<a class="paper" href="https://arxiv.org/abs/1904.10509"> Generating Long Sequences with Sparse Transformers </a><br>
	</td>
	<td align="center"><a href="http://www.wisdom.weizmann.ac.il/~/vision/courses/2021_2/ADLV/files/Efficient_Attention_Mechanism_09.05.21.pptx"> Slides </a></td>
	<td align="center"><a href="https://weizmann.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=680c64c8-e0bd-4695-afa6-ad2300e479b8"> Zoom </a></td>
	</tr>


	<tr><td colspan="5" class="schedule_week" align="center" height="16" valign="middle">Advances in Object Detection / Segmentation</td></tr>
	<tr>
    <td sdnum="1033;0;@" align="center">23/05/2021</td>
    <td align="left">Ophir Sarusi <br> Raz Yerushalmi</td> 
	<td> 
		<a class="paper" href="https://ai.facebook.com/blog/end-to-end-object-detection-with-transformers/"> DETR: End-to-End Object Detection with Transformers </a><br>
		<a class="paper" href="https://arxiv.org/abs/1808.01244"> CornerNet: Detecting Objects as Paired Keypoints </a><br>
		<a class="paper" href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Pixel_Consensus_Voting_for_Panoptic_Segmentation_CVPR_2020_paper.pdf"> Pixel Consensus Voting for Panoptic Segmentation </a><br>
		<a class="paper" href="https://arxiv.org/abs/1904.02689"> YOLACT: Real-time Instance Segmentation </a><br>
		<a class="paper" href="https://arxiv.org/abs/2012.11582"> HyperSeg: Patch-wise Hypernetwork for Real-time Semantic Segmentation </a><br>
		<a class="paper" href="https://github.com/haotian-liu/yolact_edge"> YolactEdge: Real-time Instance Segmentation on the Edge </a><br>
	</td>
	<td align="center"><a href="http://www.wisdom.weizmann.ac.il/~/vision/courses/2021_2/ADLV/files/Object_detection_and_segmentation_23-5-2021_final_version.pptx"> Slides </a></td>
	<td align="center"><a href="https://weizmann.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=d6099da7-ea2a-4e2e-b005-ad32005896e0"> Zoom </a></td>
	</tr>


	<tr><td colspan="5" class="schedule_week" align="center" height="16" valign="middle">Normalization Techniques and its importance in Generative Models</td></tr>
	<tr>
    <td sdnum="1033;0;@" align="center">30/05/2021</td>
    <td align="left">Sveta Paster <br> Ore Shtalrid</td> 
	<td> 
		<a class="paper" href="https://arxiv.org/abs/1502.03167"> Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift </a><br>
		<a class="paper" href="https://arxiv.org/abs/1607.08022"> Instance Normalization: The Missing Ingredient for Fast Stylization </a><br>
		<a class="paper" href="https://arxiv.org/abs/1703.06868"> Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization </a><br>
		<a class="paper" href="https://nvlabs.github.io/SPADE/"> SPADE: Semantic Image Synthesis with Spatially-Adaptive Normalization </a><br>
		<a class="paper" href="https://arxiv.org/abs/1805.11604"> How Does Batch Normalization Help Optimization? </a><br>
		<a class="paper" href="https://arxiv.org/abs/2102.06171"> High-Performance Large-Scale Image Recognition Without Normalization </a><br>
	</td>
	<td align="center"><a href="http://www.wisdom.weizmann.ac.il/~/vision/courses/2021_2/ADLV/files/Normalization_Techniques .pptx"> Slides </a></td>
	<td align="center"><a href="https://weizmann.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=fa4cc7aa-f4e9-4011-8321-ad3800e4eedd"> Zoom </a></td>
	</tr>


	<tr><td colspan="5" class="schedule_week" align="center" height="16" valign="middle">Neural Implicit Representations</td></tr>
	<tr>
    <td sdnum="1033;0;@" align="center">06/06/2021</td>
    <td align="left">Eyal Naor <br> Dolev Ofri</td> 
	<td> 
		<a class="paper" href="https://arxiv.org/abs/1812.03828"> Occupancy Networks: Learning 3D Reconstruction in Function Space </a><br>
		<a class="paper" href="https://arxiv.org/abs/1901.05103"> DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation </a><br>
		<a class="paper" href="https://vsitzmann.github.io/siren/"> Implicit Neural Representations with Periodic Activation Functions </a><br>
		<a class="paper" href="https://bmild.github.io/fourfeat/"> Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains </a><br>
		<a class="paper" href="https://arxiv.org/abs/2003.08934"> NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis </a><br>
		<a class="paper" href="https://nex-mpi.github.io/"> NeX: Real-time View Synthesis with Neural Basis Expansion </a><br>
	</td>
	<td align="center"><a href="http://www.wisdom.weizmann.ac.il/~/vision/courses/2021_2/ADLV/files/Neural_Implicit_Representation-Dolev_and_Eyal.pdf"> Slides </a></td>
	<td align="center"><a href="https://weizmann.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=ba2ebe99-474f-4d15-8e08-ad4000644dd1"> Zoom </a></td>
	</tr>


	<tr><td colspan="5" class="schedule_week" align="center" height="16" valign="middle">Meta Learning</td></tr>
	<tr>
    <td sdnum="1033;0;@" align="center">13/06/2021</td>
    <td align="left">Lior Magram <br> Bar Karov</td> 
	<td> 
		<a class="paper" href="https://arxiv.org/abs/2001.02905"> Fast Adaptation to Super-Resolution Networks via Meta-Learning </a><br>
		<a class="paper" href="https://arxiv.org/abs/1703.03400"> Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks </a><br>
		<a class="paper" href="https://arxiv.org/abs/2012.02189"> Learned Initializations for Optimizing Coordinate-Based Neural Representations </a><br>
		<a class="paper" href="https://arxiv.org/abs/1803.02999"> On First-Order Meta-Learning Algorithms </a><br>
	</td>
	<td align="center"><a href="http://www.wisdom.weizmann.ac.il/~/vision/courses/2021_2/ADLV/files/Meta_Learning_ADLV_130621_Lior_Magram_Bar_Karov.pptx"> Slides </a></td>
	<td align="center"><a href="https://weizmann.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=308e7017-3fb0-4afc-aa2a-ad4600de8511"> Zoom </a></td>
	</tr>

	<tr><td colspan="5" class="schedule_week" align="center" height="16" valign="middle">Deep Denoising</td></tr>
	<tr>
    <td sdnum="1033;0;@" align="center">20/06/2021</td>
    <td align="left">Alon Mamistvalov <br> Hila Naaman</td> 
	<td>
		<a class="paper" href="https://arxiv.org/pdf/1803.04189.pdf"> Noise2Noise </a><br>
		<a class="paper" href="https://arxiv.org/pdf/1811.10980.pdf"> Noise2Void </a><br>
		<a class="paper" href="https://arxiv.org/pdf/1901.11365.pdf"> Noise2Self </a><br>
		<a class="paper" href="https://openreview.net/forum?id=HJeqhA4YDS"> Denoising and Regularization via Exploiting the Structural Bias of Convolutional Generators </a><br>
		<a class="paper" href="https://openreview.net/forum?id=HJlSmC4FPS"> Robust And Interpretable Blind Image Denoising Via Bias-Free Convolutional Neural Networks </a><br>	
	</td>
	<td align="center"><a href="http://www.wisdom.weizmann.ac.il/~/vision/courses/2021_2/ADLV/files/NoisePresentation.pptx"> Slides </a></td>
	<td align="center"><a href="https://weizmann.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=f5955fc2-eead-409d-93b0-ad4e00526ab7"> Zoom </a></td>
	</tr>


	<tr><td colspan="5" class="schedule_week" align="center" height="16" valign="middle">Depth Estimation from Single Image</td></tr>
	<tr>
    <td sdnum="1033;0;@" align="center">27/06/2021</td>
    <td align="left">Narek Tumanyan <br> Navve Wasserman</td> 
	<td> 
		<a class="paper" href="http://visual.cs.ucl.ac.uk/pubs/monoDepth/"> Unsupervised Monocular Depth Estimation with Left-Right Consistency </a><br>
		<a class="paper" href="https://people.eecs.berkeley.edu/~tinghuiz/projects/SfMLearner/"> Unsupervised Learning of Depth and Ego-Motion from Video </a><br>
		<a class="paper" href="https://arxiv.org/abs/2004.15021"> Consistent Video Depth Estimation </a><br>
		<a class="paper" href="https://robust-cvd.github.io/"> Robust Consistent Video Depth Estimation </a><br>
	</td>
	<td align="center"><a href="http://www.wisdom.weizmann.ac.il/~/vision/courses/2021_2/ADLV/files/Depth_Estimation_from_Single_Image.pptx"> Slides </a></td>
	<td align="center"><a href="https://weizmann.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=20eccb40-818d-4aa2-9231-ad5400dcb178"> Zoom </a></td>
	</tr>


	<tr><td colspan="5" class="schedule_week" align="center" height="16" valign="middle">Out of  Distribution Detection</td></tr>
	<tr>
    <td sdnum="1033;0;@" align="center">04/07/2021</td>
    <td align="left">Myriam Schmidt</td> 
	<td> 
		<a class="paper" href="https://arxiv.org/abs/1610.02136"> A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks </a><br>
		<a class="paper" href="https://arxiv.org/abs/1807.03888"> A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks </a><br>
		<a class="paper" href="https://arxiv.org/abs/1706.02690"> Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks </a><br>
	</td>
	<td align="center"><a href="http://www.wisdom.weizmann.ac.il/~/vision/courses/2021_2/ADLV/files/out_of_distribution.pptx"> Slides </a></td>
	<td align="center"><a href="https://weizmann.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=2993480b-0c95-44bd-a91f-ad5b00cefcf7"> Zoom </a></td>
	</tr>


  </tbody>
</table>
<p>&nbsp;</p>
</div>


<iframe id="zotero-modal-prompt" src="./schedule_files/modalPrompt.html" style="position: fixed; top: 0px; left: unset; right: 0px; width: 100%; height: 100%; border: none; display: none; z-index: 2147483647;"></iframe>

</body>
</html>
